{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\n\nfrom tqdm import tqdm\nfrom random import choices\n\n\nimport kerastuner as kt\n\nimport gc\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"##前處理參考Bottleneck encoder + MLP + Keras Tuner 8601c5\n#Train = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n##Pandas query\n##https://blog.csdn.net/AlanGuoo/article/details/88874742\n##reset_index()\n##https://ithelp.ithome.com.tw/articles/10194006\n#Train = Train.query('date > 85').reset_index(drop = True)\n##篩選出date>85\n##reset_index()可以讓index重置成原本的樣子\n##轉型態astype\n##https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html\n#Train = Train.astype({c: np.float32 for c in Train.select_dtypes(include='float64').columns}) #limit memory use\n##dtype選擇特定條件之列:資料型別float64\n##float64->float32\n##fillna填0，mean()會自動忽略nan，inplace=True會取代原數據，就不用再丟回，ex:Train()，Train = Train.fillna(Train.mean())\n#Train.fillna(Train.mean(),inplace=True)\n#Train = Train.query('weight > 0').reset_index(drop = True)\n#Train['action'] =  (  (Train['resp_1'] > 0 ) & (Train['resp_2'] > 0 ) & (Train['resp_3'] > 0 ) & (Train['resp_4'] > 0 ) &  (Train['resp'] > 0 )   ).astype('int')\n##原數據有小數float->int\n\n#features = [c for c in Train.columns if 'feature' in c]#抓出特徵\n\n#resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']#抓出resp\n\n#X = Train[features].values #用來訓練\n#y = np.stack([(Train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n\n#f_mean = np.mean(Train[features[1:]].values,axis=0)#mean()會自動忽略nan，抓出有值的部分，索引1到最後\n##不抓features_0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features_CSV = pd.read_csv('../input/jane-street-market-prediction/features.csv')#\n##https://www.ycc.idv.tw/tensorflow-tutorial_4.html\n##features.csv抓features_0查看數值意義\n##f_features = np.mean(features_CSV[0:1].values,axis=0)\n\n#f_features = features_CSV[0:1].values\n#f_features = np.delete(f_features,[0][0])\n#print(f_features)\n#f_features = f_features.astype('int')\n#f_features\n\n##在features.csv中features_0數值都一樣，無意義，因此在train.csv裡沒用到","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#減少記憶體空間\n#Jane Street - Tensorflow Dense\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\ntrain = reduce_mem_usage(train)\nfeatures = [c for c in train.columns if 'feature' in c]#抓出特徵\n\nNAN_VALUE = -999#空值填入-999","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.astype({c: np.float32 for c in train.select_dtypes(include='float16').columns}) \ntrain = train.fillna(train.mean())\n##dtype選擇特定條件之列:資料型別float16\n##float16->float32\n##fillna填0，mean()會自動忽略nan，inplace=True會取代原數據，就不用再丟回，ex:Train()，Train = Train.fillna(Train.mean())\nf_mean = np.mean(train[features[1:]].values,axis=0)\n#mean()會自動忽略nan，抓出有值的部分，索引1到最後\n##不抓features_0\ntrain = train.query('date > 85').reset_index(drop = True)\n##篩選出date>85\n##reset_index()可以讓index重置成原本的樣子\ntrain = train[train.weight != 0]\n#0無意義\nn_folds = 5\n#折數\nseed = 2020\n#種子數\nskf = StratifiedKFold(n_splits=n_folds, shuffle=False)\n#shuffle隨機排序\n\nX = train.loc[:, features].values\n#if np.isnan(X[:, 1:].sum()):\n#    X[:, 1:] = np.nan_to_num(X[:, 1:]) + np.isnan(X[:, 1:]) * NAN_VALUE\n    \n#y = (train['resp'].values > 0).astype(int)\nresp_cols = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']#抓出resp\ny = np.stack([(train[c] > 0.000001).astype('int') for c in resp_cols]).T \n\ntrain_index, test_index = next(skf.split(X, y[:,0]))\n#train_index, test_index = next(skf.split(X, y))\nX_train, X_test = X[train_index], X[test_index]\ny_train, y_test = y[train_index], y[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Jane Street - Tensorflow Dense\nTUNNING = False\n#以下為tf.keras.layers中之應用的解釋\n#keras的layer類直接建立深度網絡中的layer\n##Input用於實例化Keras\n##BatchNormalization在每批中對上一層的激活進行歸一化，即應用一個轉換，將平均激活保持在0附近並將激活標準偏差保持在1附近\n##GaussianNoise高斯噪聲\n##Dropout輸入\n###http://man.hubwiz.com/docset/TensorFlow.docset/Contents/Resources/Documents/api_docs/python/tf/keras/layers/BatchNormalization.html\n##Dense用於添加一個全連接層\ndef create_model(hp,input_dim,output_dim):\n    inputs = tf.keras.layers.Input(input_dim)\n    x = tf.keras.layers.BatchNormalization()(inputs)\n    x = tf.keras.layers.GaussianNoise(hp.Choice('noise',[0.0,0.03,0.05]))(x)\n    x = tf.keras.layers.Dropout(hp.Choice('init_dropout',[0.0,0.3,0.5]))(x)    \n    x = tf.keras.layers.Dense(hp.Int('num_units_1', 128, 2048, 64), activation=hp.Choice('activation_1', ['tanh','relu','swish']))(x)\n    #units輸出變數\n    x = tf.keras.layers.Dropout(hp.Choice(f'dropout_1',[0.0,0.3,0.5]))(x)\n    x = tf.keras.layers.Dense(hp.Int('num_units_2', 128, 1024, 32), activation=hp.Choice('activation_2', ['tanh','relu','swish']))(x)\n    x = tf.keras.layers.Dropout(hp.Choice(f'dropout_2',[0.0,0.3,0.5]))(x)\n    x = tf.keras.layers.Dense(output_dim, activation='sigmoid')(x)\n    model = tf.keras.models.Model(inputs=inputs,outputs=x)#設定輸出入之格式\n    #compile編譯模型\n    #解釋：https://dotblogs.com.tw/greengem/2017/12/17/094023\n    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('lr',[1e-2, 1e-3, 1e-5])),\n                  #optimizer優化器選用\n                  #實現Adam算法的優化器\n                  #優化器解釋：https://keras.io/zh/optimizers/\n                  #lr為學習率\n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=hp.Choice('label_smoothing',[0.0, 0.01, 0.1])),\n                  #tf.keras.losses.BinaryCrossentropy計算真實標籤和預測標籤之間的交叉熵損失\n                  metrics=[tf.keras.metrics.AUC(name = 'auc')])\n                  #metrics監視模型並判斷性能\n                  #tf.keras.metrics.AUC通過黎曼和求出近似的AUC（曲線下的面積）\n    return model\n#Sequential將線性的層堆疊到一個tf.keras.Model\n#https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n#創建一個“Sequential”模型並添加一個Dense層作為第一層\nmodel = tf.keras.Sequential([\n    tf.keras.Input(shape = len(features)),#特徵長度\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.GaussianNoise(0.05),\n    tf.keras.layers.Dropout(0.00), \n    tf.keras.layers.Dense(2048, activation='tanh'),#雙曲正切激活函數\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1024, activation='swish'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(2048, activation='swish'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(128, activation='swish'),    \n    tf.keras.layers.Dense(5, activation = 'sigmoid')#Sigmoid 激活函数\n  ])\n#activation的相關解釋：https://keras.io/zh/activations/\n#tf.keras.layers.Dense(輸出入尺寸)\nEPOCHS = 500#訓練過程中數據將被用多少次\nBATCH_SIZE = 1024#batch_size=4096,#梯度下降\n\nif TUNNING:\n    import kerastuner as kt\n    EPOCHS = 50#訓練過程中數據將被用多少次\n    MAX_TRIAL = 20#調參過程中進行實驗的參數組合的總數目\n    model_fn = lambda hp: create_model(hp, X_train.shape[-1], y_train.shape[-1])\n    tuner = kt.tuners.BayesianOptimization(model_fn, kt.Objective('val_auc', direction='max'), MAX_TRIAL, seed = 2020)\n    #貝葉斯優化\n    tuner.search(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test),callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True)])\n    model = tuner.get_best_models()[0]\nelse:\n    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    #optimizer = tf.keras.optimizers.RMSprop()\n    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.00)\n    model.compile(loss = loss, optimizer=optimizer, metrics=[tf.keras.metrics.AUC()])\n    #觀察網絡性能指標\n    history = model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callback], validation_data=(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TUNNING:\n    #打印結果摘要\n    tuner.results_summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nfrom tqdm.notebook import tqdm\n#janestreet.competition.make_env.__called__ = False\nenv = janestreet.make_env()\n#啟動環境，JaneStreet使用該環境提供測試行。然後，它創建一個迭代器，該迭代器允許獲取您需要預測的所有行。\niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in tqdm(iter_test):\n    #循環拋出所有應該預測的行。每個test_df是一個數據框，其中包含必須預測的一行。\n    if test_df['weight'].item() > 0:\n        #如果權重為0，則預測將對分數沒有影響。因此，為了節省計算時間，我們不預測權重== 0的時間\n        x_tt = test_df.loc[:, features].values\n        #如前所述，每個test_df都包含一個數據幀。但是您的模型無法使用數據框，它需要一個數組數組……然後將其轉換\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n            #x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * NAN_VALUE\n            #如果缺少任何值，我們必須對其進行編輯。在這種情況下，我不確定目標是什麼。但是，填充缺失值的一種好\n            #方法是用平均值（對異常值敏感），中位數（對異常值不敏感）或默認值（0）代替它們。\n        action = np.mean(model(x_tt, training = False).numpy()[0])#模型結果\n        if (action > 0.5):\n            sample_prediction_df.action = 1\n        else:\n            sample_prediction_df.action = 0 \n        #等同\n        #pred = f(pred)\n        #sample_prediction_df.action = np.where(pred >= th, 1, 0).astype(int)#只是沒有=\n        ##預測動作是pred內容（將其視為具有類別0或1的概率）\n    else:\n        sample_prediction_df.action = 0 \n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# modified code for group gaps; source\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CVTuner(kt.engine.tuner.Tuner):\n    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n        val_losses = []\n        for train_indices, test_indices in splits:\n            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n            if len(X_train) < 2:\n                X_train = X_train[0]\n                X_test = X_test[0]\n            if len(y_train) < 2:\n                y_train = y_train[0]\n                y_test = y_test[0]\n            \n            model = self.hypermodel.build(trial.hyperparameters)\n            hist = model.fit(X_train,y_train,\n                      validation_data=(X_test,y_test),\n                      epochs=epochs,\n                        batch_size=batch_size,\n                      callbacks=callbacks)\n            \n            val_losses.append([hist.history[k][-1] for k in hist.history])\n        val_losses = np.asarray(val_losses)\n        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n        self.save_model(trial.trial_id, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_fn = lambda hp: create_model(hp,X.shape[-1],y.shape[-1],encoder)\n\n#tuner = CVTuner(\n#        hypermodel=model_fn,\n#        oracle=kt.oracles.BayesianOptimization(\n#        objective= kt.Objective('val_auc', direction='max'),\n#        num_initial_points=4,\n#        max_trials=20))\n\n#FOLDS = 5\n#SEED = 42\n\n#if TRAINING:\n#    gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n#    splits = list(gkf.split(y, groups=train['date'].values))\n#    tuner.search((X,),(y,),splits=splits,batch_size=4096,epochs=100,callbacks=[EarlyStopping('val_auc', mode='max',patience=3)])\n#    #hp  = tuner.get_best_hyperparameters(1)[0]\n#    pd.to_pickle(hp,f'./best_hp_{SEED}.pkl')\n#    for fold, (train_indices, test_indices) in enumerate(splits):\n#        model = model_fn(hp)\n#        X_train, X_test = X[train_indices], X[test_indices]\n#        y_train, y_test = y[train_indices], y[test_indices]\n#        model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=4096,callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\n#        model.save_weights(f'./model_{SEED}_{fold}.hdf5')\n#        model.compile(Adam(hp.get('lr')/100),loss='binary_crossentropy')\n#        model.fit(X_test,y_test,epochs=3,batch_size=4096)\n#        model.save_weights(f'./model_{SEED}_{fold}_finetune.hdf5')\n#    tuner.results_summary()\n#else:\n    #models = []\n    #hp = pd.read_pickle(f'../input/vae223/best_hp_{SEED}.pkl')\n    #for f in range(FOLDS):\n    #    model = model_fn(hp)\n    #    if USE_FINETUNE:\n    #        model.load_weights(f'../input/vae223/model_{SEED}_{f}_finetune.hdf5')\n    #        model.load_weights(f'./model_{SEED}_{f}_finetune.hdf5')\n    #    else:\n    #        #model.load_weights(f'../input/vae223/model_{SEED}_{f}.hdf5')\n    #        model.load_weights(f'./model_{SEED}_{f}.hdf5')\n    #    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=333) \n\n#print(X_train.shape, X_test.shape, y_train.shape, y_test.shape) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://towardsdatascience.com/multiclass-classification-and-information-bottleneck-an-example-using-keras-5591b9a2c000\n#Bottleneck\n#from keras import models\n#from keras import layers\n\n#model = models.Sequential()\n#model.add(layers.Dense(64, activation='relu', input_shape = (130,)))#數值為X.shape\n#model.add(layers.Dense(64, activation='relu'))#使用具有64個隱藏單元的圖層\n#model.add(layers.Dense(5, activation='softmax'))#數值為y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#編譯模型\n#model.compile(optimizer='rmsprop',\n#             loss='categorical_crossentropy',\n#             metrics=['accuracy'])\n#解釋：https://dotblogs.com.tw/greengem/2017/12/17/094023\n#優化器選用rmsprop\n#損失函數當預測值與實際值愈相近，損失函數就愈小，反之差距很大，就會更影響損失函數的值，這篇文章 主張要用 Cross Entropy 取代 MSE，\n#因為，在梯度下時，Cross Entropy 計算速度較快\n#監視accuracy模型\n#from tensorflow.keras.optimizers import Adam\n#from tensorflow.keras.losses import BinaryCrossentropy\n#model.compile(optimizer=Adam(0.001),\n#              loss='binary_crossentropy',\n#              metrics=[tf.keras.metrics.AUC(name = 'auc')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#取得驗證集\n# Validation samples\n#X_val = X_train[:1000]\n#partial_X_train = X_train[1000:]\n\n# Validation labels\n#y_val = y_train[:100]\n#partial_y_train = y_train[1000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras.callbacks import ModelCheckpoint, EarlyStopping\n#訓練模型\n#epochs, batch_size, validation_split參考Bottleneck encoder + MLP + Keras Tuner 8601c5\n#model_train = model.fit(partial_X_train,\n#                   partial_y_train,\n#                   epochs=100,#epochs=20#epochs=9\n#                   batch_size=4096,#batch_size=512\n#                   #epochs=30,#epochs=20#epochs=9\n#                   #batch_size=128,#batch_size=512\n#                   #validation_split=0.1,\n#                   #callbacks=[EarlyStopping('val_loss',patience=10,restore_best_weights=True)]\n#                   validation_data=(X_val, y_val)\n#                   )\n#model_train = model.fit(X_train,y_train,\n#              validation_data=(X_test,y_test),#驗證集\n#              epochs=1000,#訓練過程中數據將被用多少次\n#              batch_size=4096,#梯度下降\n#              callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\n              #回呼函數\n    #EarlyStopping是Callbacks的一種，callbacks用於指定在每個epoch開始和結束的時候進行哪種特定操作。\n    #Callbacks中有一些設置好的接口，可以直接使用，如’acc’,’val_acc’,’loss’和’val_loss’等等。\n    #patience：能夠容忍多少個epoch內都沒有improvement。這個設置其實是在抖動和真正的準確率下降之間做tradeoff。\n    #mode: 就’auto’, ‘min’, ‘,max’三個可能。如果知道是要上升還是下降，建議設置一下。monitor是’acc’，所以mode=’max’。\n    #restore_best_weights：. 如果要在停止後儲存最佳權重，請將此引數設定為True. \n    #https://www.mdeditor.tw/pl/2oB4/zh-tw\n    #https://medium.com/ai%E5%8F%8D%E6%96%97%E5%9F%8E/learning-model-earlystopping%E4%BB%8B%E7%B4%B9-%E8%BD%89%E9%8C%84-f364f4f220fb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#將模型儲存至 HDF5 檔案中\n#https://blog.gtwang.org/programming/keras-save-and-load-model-tutorial/\n#model.save(f'./my_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras.models import load_model\n\n# 刪除既有模型變數\n#del model \n\n# 載入模型\n#model = load_model('my_model.h5')\n\n# 驗證模型\n#score = model.evaluate(X_test, y_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#觀察網絡性能指標\n#history_dict = model_train.history\n#history_dict.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#訓練和驗證準確性\n#import matplotlib.pyplot as plt\n#%matplotlib inline\n#loss_values = history_dict['loss']\n#val_loss_values = history_dict['val_loss']\n\n#epochs = range(1, len(loss_values) + 1)\n\n#plt.plot(epochs, loss_values, 'bo', label=\"Training Loss\")\n#plt.plot(epochs, val_loss_values, 'b', label=\"Validation Loss\")\n\n#plt.title('Training and Validation Loss')\n#plt.xlabel('Epochs')\n#plt.ylabel('Loss Value')\n#plt.legend()\n\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training and Validation Accuracy\n\n#acc_values = history_dict['accuracy']\n#val_acc_values = history_dict['val_accuracy']\n\n#acc_values = history_dict['auc']\n#val_acc_values = history_dict['val_auc']\n\n#epochs = range(1, len(loss_values) + 1)\n\n#plt.plot(epochs, acc_values, 'ro', label=\"Training Accuracy\")\n#plt.plot(epochs, val_acc_values, 'r', label=\"Validation Accuracy\")\n\n#plt.title('Training and Validation Accuraccy')\n#plt.xlabel('Epochs')\n#plt.ylabel('Accuracy')\n#plt.legend()\n\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#results = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=333)\n\n#print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n#資料分割參考：Jane Street - Tensorflow Dense\n#from sklearn.model_selection import StratifiedKFold\n\n#skf = StratifiedKFold(n_splits=5, shuffle=False)\n\n#train_index, test_index = next(skf.split(X, y[:,0]))\n#train_index, test_index = next(skf.split(X, y))\n#X_train, X_test = X[train_index], X[test_index]\n#y_train, y_test = y[train_index], y[test_index]\n#print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://github.com/jorgesleonel/Multilayer-Perceptron/blob/master/Basic%20Multi-Layer%20Perceptron.ipynb\n\n#import sklearn\n#from sklearn import preprocessing\n##from sklearn.model_selection import train_test_split \n#from sklearn.preprocessing import StandardScaler  \n#from sklearn.neural_network import MLPClassifier \n#from sklearn.metrics import classification_report, confusion_matrix\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n#特徵縮放\n#scaler = StandardScaler()  \n#scaler.fit(X_train)\n#X_train = scaler.transform(X_train)  \n#X_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MLP多層感測器\n#mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)  \n#mlp.fit(X_train, y_train.values.ravel())\n#mlp.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#預測結果\n#predictions = mlp.predict(X_test)\n#print(predictions)\n#查看效能\n#print(confusion_matrix(y_test,predictions))  \n#print(confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1)))\n#print(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#多類分類器與信息瓶頸之分數為0\n#改用xgboost->但跑太久或不動\n#https://www.itread01.com/articles/1476146171.html\n#import xgboost as xgb\n#from xgboost.sklearn import XGBClassifier\n#https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC5-2%E8%AC%9B-kaggle%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E7%AB%B6%E8%B3%BD%E7%A5%9E%E5%99%A8xgboost%E4%BB%8B%E7%B4%B9-1c8f55cffcc\n#xgbc = XGBClassifier()\n#xgbc.fit(X_train, y_train)\n\n#pred_xgbc = xgbc.pred(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://ithelp.ithome.com.tw/articles/10197461\n#交叉驗證\n#from sklearn.model_selection import cross_val_score#評分資料準確度的\n#from sklearn import datasets\n#from sklearn.model_selection import train_test_split\n#from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#knn = KNeighborsClassifier(n_neighbors=10)#找尋附近10鄰居\n#scores = cross_val_score(knn,X,y,cv=5,scoring='accuracy')#分5組\n#print(scores)\n#print(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##https://kegui.medium.com/how-to-do-cross-validation-in-keras-tuner-db4b2dbe079a\n#import kerastuner\n#from sklearn import model_selection\n\n#class CVTuner(kerastuner.engine.tuner.Tuner):\n#    def run_trial(self, trial, x, y, batch_size=32, epochs=1):\n#        cv = model_selection.KFold(5)\n#        val_losses = []\n#        for train_indices, test_indices in cv.split(x):\n#            x_train, x_test = x[train_indices], x[test_indices]\n#            y_train, y_test = y[train_indices], y[test_indices]\n#            model = self.hypermodel.build(trial.hyperparameters)\n#            model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n#            val_losses.append(model.evaluate(x_test, y_test))\n#        self.oracle.update_trial(trial.trial_id, {'val_loss': np.mean(val_losses)})\n#        self.save_model(trial.trial_id, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def create_model(hp,input_dim,output_dim,encoder):\n#    inputs = Input(input_dim)\n    \n#    x = encoder(inputs)\n#    x = Concatenate()([x,inputs]) #use both raw and encoded features\n#    x = BatchNormalization()(x)\n#    x = Dropout(hp.Float('init_dropout',0.0,0.5))(x)\n    \n#    for i in range(hp.Int('num_layers',1,3)):\n#        x = Dense(hp.Int('num_units_{i}',64,256))(x)\n#        x = BatchNormalization()(x)\n#        x = Lambda(tf.keras.activations.swish)(x)\n#        x = Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\n#    x = Dense(output_dim,activation='sigmoid')(x)\n#    model = Model(inputs=inputs,outputs=x)\n#    model.compile(optimizer=Adam(hp.Float('lr',0.00001,0.1,default=0.001)),loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing',0.0,0.1)),metrics=[tf.keras.metrics.AUC(name = 'auc')])\n#    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n#from sklearn.model_selection import KFold\n#FOLDS = 5\n#kf = KFold(n_splits=FOLDS)\n#kf.get_n_splits(X)\n\n#print(kf)\n\n#from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n#from tensorflow.keras.models import Model, Sequential\n#from tensorflow.keras.losses import BinaryCrossentropy\n#from tensorflow.keras.optimizers import Adam\n#from tensorflow.keras.callbacks import EarlyStopping\n#from tensorflow.keras.layers.experimental.preprocessing import Normalization\n#from sklearn.model_selection import GroupKFold\n\n#from tqdm import tqdm\n#from random import choices\n#import kerastuner as kt\n#inputs = Input(X.shape[-1])\n##https://keras.io/api/models/model/\n#x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n\n#model = Model(inputs=inputs,outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x))\n#model.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n#              loss='BinaryCrossentropy',metrics=[tf.keras.metrics.AUC(name = 'auc')])\n\n#my_model = lambda hp: create_model(hp,X.shape[-1],y.shape[-1],encoder)\n\n#models = []\n#fold = 0\n#for train_index, test_index in kf.split(X):\n#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#    X_train, X_test = X[train_index], X[test_index]\n#    y_train, y_test = y[train_index], y[test_index]\n    \n#    model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=4096,callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\n#    model.save_weights(f'./model_{42}_{fold}.hdf5')\n#    model.compile(Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),loss='binary_crossentropy')\n#    model.fit(X_test,y_test,epochs=3,batch_size=4096)\n#    #model.save_weights(f'./model_{42}_{fold}_finetune.hdf5')\n#    models.append(model)\n#    fold = fold+1\n#for f in range(FOLDS):\n#    model = model.load_weights(f'./model_{42}_{f}_finetune.hdf5')\n#    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#models = models[-2:]\n#f = np.median\n#th = 0.5\n#import janestreet\n#janestreet.make_env.__called__ = False\n#env = janestreet.make_env()\n##啟動環境，JaneStreet使用該環境提供測試行。然後，它創建一個迭代器，該迭代器允許獲取您需要預測的所有行。\n#for (test_df, sample_prediction_df) in tqdm(env.iter_test()):\n##循環拋出所有應該預測的行。每個test_df是一個數據框，其中包含必須預測的一行。\n#    if test_df['weight'].item() > 0:\n#    #如果權重為0，則預測將對分數沒有影響。因此，為了節省計算時間，我們不預測權重== 0的時間\n    \n#        X_tt = test_df.loc[:, features].values\n#        #如前所述，每個test_df都包含一個數據幀。但是您的模型無法使用數據框，它需要一個數組數組……然後將其轉換\n#        if np.isnan(X_tt[:, 1:].sum()):\n#            X_tt[:, 1:] = np.nan_to_num(X_tt[:, 1:]) + np.isnan(X_tt[:, 1:]) * f_mean\n#            #如果缺少任何值，我們必須對其進行編輯。在這種情況下，我不確定目標是什麼。但是，填充缺失值的一種好\n#            #方法是用平均值（對異常值敏感），中位數（對異常值不敏感）或默認值（0）代替它們。\n#        pred = np.mean([model(X_tt, training = False).numpy() for model in models],axis=0)\n#        #pred = np.mean([model(X_tt, training = False).numpy()],axis=0)#模型\n#        #pred = np.mean(predictions,axis=0)#模型\n#        pred = f(pred)\n#        sample_prediction_df.action = np.where(pred >= th, 1, 0).astype(int)\n#        #預測動作是pred內容（將其視為具有類別0或1的概率）\n#    else:\n#        sample_prediction_df.action = 0\n#    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}